/mnt/home/lichenni/anaconda3/envs/oort/lib/python3.6/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.
Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.
  from numba.decorators import jit as optional_jit
2021-07-02:20:36:40,297 INFO     [param_server.py:12] End up with cuda device tensor([0.5935], device='cuda:0')
2021-07-02:20:36:40,699 INFO     [param_server.py:532] ====Start to initialize dataset
2021-07-02:20:36:40,701 INFO     [flLibs.py:59] ====Initialize the model
/mnt/home/lichenni/anaconda3/envs/oort/lib/python3.6/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.
Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.
  from numba.decorators import jit as optional_jit
2021-07-02:20:37:10,362 INFO     [learner.py:14] End up with cuda device tensor([0.4680], device='cuda:0')
2021-07-02:20:37:10,365 INFO     [learner.py:40] ===== Experiment start on : nvl-002=====
2021-07-02:20:37:10,769 INFO     [learner.py:703] ====Start to initialize dataset
2021-07-02:20:37:10,770 INFO     [flLibs.py:59] ====Initialize the model
2021-07-02:20:37:11,350 INFO     [learner.py:726] ==== Starting training data partitioner =====
2021-07-02:20:37:11,358 INFO     [divide_data.py:94] ====Initiating DataPartitioner takes 0.0073163509368896484 s

2021-07-02:20:37:11,359 INFO     [learner.py:729] ==== Finished training data partitioner =====
2021-07-02:20:37:11,401 INFO     [divide_data.py:350] ========= Start of Random Partition =========

2021-07-02:20:37:11,414 INFO     [divide_data.py:363] ====Try to remove clients w/ less than 257 samples, and remove 0 samples
2021-07-02:20:37:11,499 INFO     [divide_data.py:471] Raw class per worker is : array([[33., 34., 13., ...,  0.,  0.,  0.],
       [35., 40., 18., ...,  0.,  0.,  0.],
       [45., 33., 18., ...,  0.,  0.,  0.],
       ...,
       [31., 51., 15., ...,  0.,  0.,  0.],
       [33., 41., 22., ...,  0.,  0.,  0.],
       [36., 47., 12., ...,  0.,  0.,  0.]])

2021-07-02:20:37:11,500 INFO     [divide_data.py:472] ========= End of Class/Worker =========

2021-07-02:20:37:11,503 INFO     [learner.py:83] ====Save obs_client====
2021-07-02:20:37:12,369 INFO     [param_server.py:78] ====Info of all feasible clients {'total_feasible_clients': 100, 'total_length': 32800}
2021-07-02:20:37:12,643 INFO     [param_server.py:139] ====PS: get in run()
2021-07-02:20:37:12,661 INFO     [learner.py:742] ==== Starting testing data partitioner =====
2021-07-02:20:37:12,662 INFO     [divide_data.py:94] ====Initiating DataPartitioner takes 0.00022220611572265625 s

2021-07-02:20:37:12,662 INFO     [learner.py:744] ==== Finished testing data partitioner =====
2021-07-02:20:37:12,663 INFO     [divide_data.py:350] ========= Start of Random Partition =========

2021-07-02:20:37:12,665 INFO     [divide_data.py:471] Raw class per worker is : array([[ 74.,  70.,  93.,  90.,  71.,  98., 112.,  61., 101., 105.,   0.,
          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]])

2021-07-02:20:37:12,666 INFO     [divide_data.py:472] ========= End of Class/Worker =========

2021-07-02:20:37:12,668 INFO     [learner.py:452] ====Worker: Start running
2021-07-02:20:37:13,12 INFO     [learner.py:490] 
Namespace(adam_epsilon=1e-08, backend='nccl', batch_size=256, bidirectional=True, blacklist_max_len=0.3, blacklist_rounds=-1, block_size=64, cache_dir=None, capacity_bin=True, clf_block_size=100, client_path='/mnt/ufs18/nodr/home/lichenni/projects/FedScale/dataset/data/device_info/client_profile.pkl', clip_bound=0.98, clock_factor=6.092057761732853, conf_path='~/dataset/', config_name=None, cut_off_util=0.4, data_dir='/mnt/ufs18/nodr/home/lichenni/projects/FedScale/dataset/google_speech', data_mapfile='/mnt/ufs18/nodr/home/lichenni/projects/FedScale/dataset/google_speech/clientDataMap', data_set='google_speech', decay_epoch=15.0, decay_factor=0.95, display_step=20, do_eval=False, do_train=False, dump_epoch=1000, duplicate_data=1, enable_debug=True, enable_importance=True, enforce_random=True, epochs=1, eval_all_checkpoints=False, eval_data_file='', eval_interval=1, eval_interval_prior=9999999, evaluate_during_training=False, exploration_alpha=0.3, exploration_decay=0.95, exploration_factor=0.9, exploration_min=0.2, filter_class=0, filter_less=257, filter_more=100000.0, finetune=False, fixed_clients=False, force_read=False, forward_pass=False, fp16=False, fp16_opt_level='O1', full_gradient_interval=20, gpu_device=0, gradient_accumulation_steps=1, gradient_policy='', hetero_allocation='1.0-1.0-1.0-1.0-1.0-1.0', heterogeneity=1.0, hidden_layers=7, hidden_size=256, input_dim=0, is_even_avg=True, job_name='google_speech', labels_path='labels.json', learners='1', learning_rate=0.04, line_by_line=False, load_epoch=1, load_model=False, load_time_stamp='0615_194942', local_rank=-1, log_path='/mnt/home/lichenni/projects/Oort/training/evals', logging_steps=500, loss_decay=0.2, malicious_clients=0, manager_port=37543, max_grad_norm=1.0, max_iter_store=100, max_steps=-1, min_learning_rate=0.0001, mlm=True, mlm_probability=0.1, model='resnet34', model_avg=True, model_name_or_path=None, model_path=None, model_size=65536, model_type='', no_cuda=False, noise_dir=None, noise_factor=0, noise_max=0.5, noise_min=0.0, noise_prob=0.4, num_class=20, num_classes=10, num_loaders=2, num_train_epochs=1.0, output_dim=0, output_dir=None, overcommit=1.1, overwrite_cache=False, overwrite_output_dir=False, pacer_delta=30.0, pacer_step=20, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, proxy_avg=False, proxy_mu=0.1, ps_ip='nvl-002', ps_port='54007', read_models_path=False, release_cache=False, resampling_interval=1, rnn_type='lstm', round_penalty=2.0, round_threshold=40.0, run_all=False, sample_mode='random', sample_rate=16000, sample_seed=233, sample_window=5.0, sampler_path=None, save_path='./', save_steps=500, save_total_limit=None, score_mode='loss', seed=42, sequential='0', server_ip='', server_port='', should_continue=False, single_sim=0, skip_partition=False, sleep_up=0, spec_augment=False, speed_volume_perturb=False, stale_threshold=0, task='speech', test_bsz=256, test_interval=20, test_manifest='data/test_manifest.csv', test_only=False, test_ratio=1.0, test_train_data=False, this_rank=1, threads=4, time_stamp='0702_203636', timeout=9999999, to_device='cuda', tokenizer_name=None, total_worker=100, train_data_file='', train_manifest='data/train_manifest.csv', upload_epoch=20, user_trace=None, validate_interval=999999, vocab_tag_size=500, vocab_token_size=10000, warmup_steps=0, weight_decay=0.0, window='hamming', window_size=0.02, window_stride=0.01, yogi_beta=0.999, yogi_beta2=-1, yogi_eta=0.005, yogi_tau=0.001, zipf_alpha='5')

2021-07-02:20:37:13,481 INFO     [learner.py:533] ====Start train round 1
2021-07-02:20:37:13,802 INFO     [learner.py:159] Start to run client 1 on rank 1...
====Worker: init_myprocesses
Begin!
2021-07-02:20:43:47,547 INFO     [learner.py:401] ====Save obs_importance====
2021-07-02:20:43:47,863 INFO     [learner.py:446] Completed to run client 1
2021-07-02:20:43:48,482 INFO     [learner.py:609] ====Pushing takes 0.5984728336334229 s
2021-07-02:20:43:49,433 INFO     [param_server.py:239] ====Start to merge models
2021-07-02:20:43:50,226 INFO     [param_server.py:297] ====Done handling rank 1, with ratio 1.0, now collected 1 clients
2021-07-02:20:43:50,226 INFO     [param_server.py:318] ====After aggregation in epoch: 0, virtual_clock: 0.0, top_1: : 0.0 % (0.0), top_5: : 0.0 % (0.0), test loss: 0.0, test len: 1.0
2021-07-02:20:43:50,229 INFO     [param_server.py:347] Lock worker 1 with localStep 1 , while globalStep is 1

2021-07-02:20:43:50,229 INFO     [param_server.py:380] ====Epoch 2 completes 1 clients with loss 7.134458111185666, sampled rewards are: 
 {1: 0} 
==========
2021-07-02:20:43:50,230 INFO     [param_server.py:389] ====Start to sample for epoch 2, global virtualClock: 0.0, round_duration: 0.0
2021-07-02:20:43:50,231 INFO     [param_server.py:409] ====Try to resample clients, final takes: 
 [74, 23, 33, 99, 7, 31, 77, 48, 62, 55, 30, 17, 54, 4, 89, 39, 71, 64, 83, 82, 51, 92, 34, 52, 6, 93, 50, 21, 68, 11, 19, 70, 43, 20, 1, 9, 18, 96, 60, 32, 88, 47, 41, 16, 67, 69, 94, 10, 25, 8, 76, 24, 97, 100, 42, 2, 65, 12, 29, 86, 61, 80, 87, 63, 72, 57, 5, 26, 98, 36, 53, 95, 84, 40, 90, 66, 56, 13, 58, 15, 22, 75, 27, 91, 49, 3, 81, 85, 35, 37, 28, 78, 73, 46, 45, 44, 38, 79, 14, 59]
2021-07-02:20:43:50,500 INFO     [param_server.py:484] Epoch is done: 2
2021-07-02:20:43:52,883 INFO     [utils_model.py:294] Rank 1: Test set: Average loss: 53600.1221, Top-1 Accuracy: 61.0/875 (0.0697), Top-5 Accuracy: 0.5234
2021-07-02:20:43:52,884 INFO     [learner.py:650] After aggregation epoch 1, CumulTime 400.2149, eval_time 2.5006, test_loss 53600.1221, test_accuracy 0.0697, test_5_accuracy 0.5234 

2021-07-02:20:43:53,154 INFO     [learner.py:661] ====Dump model successfully
Traceback (most recent call last):
  File "/mnt/home/lichenni/projects/Oort/training/learner.py", line 763, in <module>
    run, args.backend, client_cfg)
  File "/mnt/home/lichenni/projects/Oort/training/learner.py", line 101, in init_myprocesses
    fn(rank, model, q, param_q, stop_flag, client_cfg)
  File "/mnt/home/lichenni/projects/Oort/training/learner.py", line 673, in run
    queue.put({rank: [None, None, None, True, -1, -1]})
  File "<string>", line 2, in put
  File "/mnt/home/lichenni/anaconda3/envs/oort/lib/python3.6/multiprocessing/managers.py", line 757, in _callmethod
    kind, result = conn.recv()
  File "/mnt/home/lichenni/anaconda3/envs/oort/lib/python3.6/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/mnt/home/lichenni/anaconda3/envs/oort/lib/python3.6/multiprocessing/connection.py", line 407, in _recv_bytes
    buf = self._recv(4)
  File "/mnt/home/lichenni/anaconda3/envs/oort/lib/python3.6/multiprocessing/connection.py", line 383, in _recv
    raise EOFError
EOFError
