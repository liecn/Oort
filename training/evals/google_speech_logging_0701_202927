/mnt/home/lichenni/anaconda3/envs/oort/lib/python3.6/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.
Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.
  from numba.decorators import jit as optional_jit
2021-07-01:20:29:37,279 INFO     [param_server.py:11] End up with cuda device tensor([0.5861], device='cuda:0')
2021-07-01:20:29:37,720 INFO     [param_server.py:532] ====Start to initialize dataset
2021-07-01:20:29:37,723 INFO     [flLibs.py:59] ====Initialize the model
/mnt/home/lichenni/anaconda3/envs/oort/lib/python3.6/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.
Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.
  from numba.decorators import jit as optional_jit
2021-07-01:20:29:43,619 INFO     [learner.py:14] End up with cuda device tensor([0.6409], device='cuda:0')
2021-07-01:20:29:43,622 INFO     [learner.py:40] ===== Experiment start on : dev-amd20-v100=====
2021-07-01:20:29:44,41 INFO     [learner.py:706] ====Start to initialize dataset
2021-07-01:20:29:44,43 INFO     [flLibs.py:59] ====Initialize the model
2021-07-01:20:29:45,32 INFO     [learner.py:729] ==== Starting training data partitioner =====
2021-07-01:20:29:45,56 INFO     [divide_data.py:94] ====Initiating DataPartitioner takes 0.018471956253051758 s

2021-07-01:20:29:45,57 INFO     [learner.py:732] ==== Finished training data partitioner =====
2021-07-01:20:29:45,93 INFO     [divide_data.py:349] ========= Start of Random Partition =========

2021-07-01:20:29:45,97 INFO     [divide_data.py:362] ====Try to remove clients w/ less than 257 samples, and remove 0 samples
2021-07-01:20:29:45,171 INFO     [divide_data.py:470] Raw class per worker is : array([[33., 34., 13., ...,  0.,  0.,  0.],
       [35., 40., 18., ...,  0.,  0.,  0.],
       [45., 33., 18., ...,  0.,  0.,  0.],
       ...,
       [31., 51., 15., ...,  0.,  0.,  0.],
       [33., 41., 22., ...,  0.,  0.,  0.],
       [36., 47., 12., ...,  0.,  0.,  0.]])

2021-07-01:20:29:45,172 INFO     [divide_data.py:471] ========= End of Class/Worker =========

2021-07-01:20:29:46,268 INFO     [param_server.py:78] ====Info of all feasible clients {'total_feasible_clients': 100, 'total_length': 32800}
2021-07-01:20:29:46,494 INFO     [param_server.py:139] ====PS: get in run()
2021-07-01:20:29:46,495 INFO     [learner.py:745] ==== Starting testing data partitioner =====
2021-07-01:20:29:46,497 INFO     [divide_data.py:94] ====Initiating DataPartitioner takes 0.0004057884216308594 s

2021-07-01:20:29:46,497 INFO     [learner.py:747] ==== Finished testing data partitioner =====
2021-07-01:20:29:46,499 INFO     [divide_data.py:349] ========= Start of Random Partition =========

2021-07-01:20:29:46,502 INFO     [divide_data.py:470] Raw class per worker is : array([[ 74.,  70.,  93.,  90.,  71.,  98., 112.,  61., 101., 105.,   0.,
          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]])

2021-07-01:20:29:46,503 INFO     [divide_data.py:471] ========= End of Class/Worker =========

2021-07-01:20:29:46,507 INFO     [learner.py:455] ====Worker: Start running
2021-07-01:20:29:46,602 INFO     [learner.py:493] 
Namespace(adam_epsilon=1e-08, backend='nccl', batch_size=256, bidirectional=True, blacklist_max_len=0.3, blacklist_rounds=-1, block_size=64, cache_dir=None, capacity_bin=True, clf_block_size=100, client_path='/mnt/ufs18/nodr/home/lichenni/projects/FedScale/dataset/data/device_info/client_profile.pkl', clip_bound=0.98, clock_factor=6.092057761732853, conf_path='~/dataset/', config_name=None, cut_off_util=0.4, data_dir='/mnt/ufs18/nodr/home/lichenni/projects/FedScale/dataset/google_speech', data_mapfile='/mnt/ufs18/nodr/home/lichenni/projects/FedScale/dataset/google_speech/clientDataMap', data_set='google_speech', decay_epoch=15.0, decay_factor=0.95, display_step=20, do_eval=False, do_train=False, dump_epoch=1000, duplicate_data=1, enable_debug=True, enable_importance=False, enforce_random=True, epochs=1, eval_all_checkpoints=False, eval_data_file='', eval_interval=1, eval_interval_prior=9999999, evaluate_during_training=False, exploration_alpha=0.3, exploration_decay=0.95, exploration_factor=0.9, exploration_min=0.2, filter_class=0, filter_less=257, filter_more=100000.0, finetune=False, fixed_clients=False, force_read=False, forward_pass=False, fp16=False, fp16_opt_level='O1', full_gradient_interval=20, gpu_device=0, gradient_accumulation_steps=1, gradient_policy='', hetero_allocation='1.0-1.0-1.0-1.0-1.0-1.0', heterogeneity=1.0, hidden_layers=7, hidden_size=256, input_dim=0, is_even_avg=True, job_name='google_speech', labels_path='labels.json', learners='1', learning_rate=0.04, line_by_line=False, load_epoch=1, load_model=False, load_time_stamp='0615_194942', local_rank=-1, log_path='/mnt/home/lichenni/projects/Oort/training/evals', logging_steps=500, loss_decay=0.2, malicious_clients=0, manager_port=6557, max_grad_norm=1.0, max_iter_store=100, max_steps=-1, min_learning_rate=0.0001, mlm=True, mlm_probability=0.1, model='resnet34', model_avg=True, model_name_or_path=None, model_path=None, model_size=65536, model_type='', no_cuda=False, noise_dir=None, noise_factor=0, noise_max=0.5, noise_min=0.0, noise_prob=0.4, num_class=20, num_classes=10, num_loaders=2, num_train_epochs=1.0, output_dim=0, output_dir=None, overcommit=1.1, overwrite_cache=False, overwrite_output_dir=False, pacer_delta=30.0, pacer_step=20, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, proxy_avg=False, proxy_mu=0.1, ps_ip='dev-amd20-v100', ps_port='20140', read_models_path=False, release_cache=False, resampling_interval=1, rnn_type='lstm', round_penalty=2.0, round_threshold=40.0, run_all=False, sample_mode='random', sample_rate=16000, sample_seed=233, sample_window=5.0, sampler_path=None, save_path='./', save_steps=500, save_total_limit=None, score_mode='loss', seed=42, sequential='0', server_ip='', server_port='', should_continue=False, single_sim=0, skip_partition=False, sleep_up=0, spec_augment=False, speed_volume_perturb=False, stale_threshold=0, task='speech', test_bsz=256, test_interval=20, test_manifest='data/test_manifest.csv', test_only=False, test_ratio=1.0, test_train_data=False, this_rank=1, threads=4, time_stamp='0701_202927', timeout=9999999, to_device='cuda', tokenizer_name=None, total_worker=100, train_data_file='', train_manifest='data/train_manifest.csv', upload_epoch=50, user_trace=None, validate_interval=999999, vocab_tag_size=500, vocab_token_size=10000, warmup_steps=0, weight_decay=0.0, window='hamming', window_size=0.02, window_stride=0.01, yogi_beta=0.999, yogi_beta2=-1, yogi_eta=0.005, yogi_tau=0.001, zipf_alpha='5')

2021-07-01:20:29:46,890 INFO     [learner.py:536] ====Start train round 1
2021-07-01:20:29:47,4 INFO     [learner.py:156] Start to run client 1 on rank 1...
====Worker: init_myprocesses
Begin!
2021-07-01:20:39:55,897 INFO     [learner.py:404] ====Save obs_importance====
2021-07-01:20:39:56,246 INFO     [learner.py:449] Completed to run client 1
2021-07-01:20:39:56,724 INFO     [learner.py:612] ====Pushing takes 0.4748871326446533 s
2021-07-01:20:39:57,699 INFO     [param_server.py:239] ====Start to merge models
2021-07-01:20:39:58,130 INFO     [param_server.py:297] ====Done handling rank 1, with ratio 1.0, now collected 1 clients
2021-07-01:20:39:58,131 INFO     [param_server.py:318] ====After aggregation in epoch: 0, virtual_clock: 0.0, top_1: : 0.0 % (0.0), top_5: : 0.0 % (0.0), test loss: 0.0, test len: 1.0
2021-07-01:20:39:58,138 INFO     [param_server.py:347] Lock worker 1 with localStep 1 , while globalStep is 1

2021-07-01:20:39:58,138 INFO     [param_server.py:380] ====Epoch 2 completes 1 clients with loss 7.134461672105567, sampled rewards are: 
 {1: 0} 
==========
2021-07-01:20:39:58,139 INFO     [param_server.py:389] ====Start to sample for epoch 2, global virtualClock: 0.0, round_duration: 0.0
2021-07-01:20:39:58,152 INFO     [param_server.py:409] ====Try to resample clients, final takes: 
 [74, 99, 23, 33, 31, 7, 62, 89, 17, 54, 77, 48, 30, 55, 39, 71, 64, 92, 4, 21, 50, 70, 83, 34, 68, 51, 60, 82, 52, 93, 1, 43, 6, 9, 20, 11, 19, 96, 18, 32, 88, 67, 47, 41, 16, 94, 69, 10, 25, 76, 8, 86, 24, 97, 100, 29, 42, 65, 12, 2, 61, 80, 87, 63, 72, 57, 26, 5, 98, 36, 95, 53, 84, 40, 13, 90, 66, 56, 58, 15, 75, 22, 27, 91, 49, 35, 3, 81, 85, 37, 28, 78, 73, 46, 45, 44, 38, 79, 14, 59]
2021-07-01:20:39:58,536 INFO     [param_server.py:484] Epoch is done: 2
2021-07-01:20:40:00,437 INFO     [utils_model.py:294] Rank 1: Test set: Average loss: 2573.8851, Top-1 Accuracy: 117.0/875 (0.1337), Top-5 Accuracy: 0.4697
2021-07-01:20:40:00,439 INFO     [learner.py:653] After aggregation epoch 1, CumulTime 613.9307, eval_time 2.1417, test_loss 2573.8851, test_accuracy 0.1337, test_5_accuracy 0.4697 

2021-07-01:20:40:00,856 INFO     [learner.py:664] ====Dump model successfully
Traceback (most recent call last):
  File "/mnt/home/lichenni/projects/Oort/training/learner.py", line 766, in <module>
    run, args.backend, client_cfg)
  File "/mnt/home/lichenni/projects/Oort/training/learner.py", line 98, in init_myprocesses
    fn(rank, model, q, param_q, stop_flag, client_cfg)
  File "/mnt/home/lichenni/projects/Oort/training/learner.py", line 676, in run
    queue.put({rank: [None, None, None, True, -1, -1]})
  File "<string>", line 2, in put
  File "/mnt/home/lichenni/anaconda3/envs/oort/lib/python3.6/multiprocessing/managers.py", line 757, in _callmethod
    kind, result = conn.recv()
  File "/mnt/home/lichenni/anaconda3/envs/oort/lib/python3.6/multiprocessing/connection.py", line 250, in recv
    buf = self._recv_bytes()
  File "/mnt/home/lichenni/anaconda3/envs/oort/lib/python3.6/multiprocessing/connection.py", line 407, in _recv_bytes
    buf = self._recv(4)
  File "/mnt/home/lichenni/anaconda3/envs/oort/lib/python3.6/multiprocessing/connection.py", line 383, in _recv
    raise EOFError
EOFError
